"""
RAG Router - Retrieval Augmented Generation
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional
from ..core.database import get_supabase_client
from ..core.config import Settings
import logging
import openai

router = APIRouter()
logger = logging.getLogger(__name__)
settings = Settings()

# Configure OpenAI
openai.api_key = settings.OPENAI_API_KEY


class QueryRequest(BaseModel):
    """Query request model"""
    question: str
    course_id: Optional[str] = None
    material_id: Optional[str] = None


@router.post("/query", response_model=dict)
async def query_materials(request: QueryRequest):
    """Query materials using RAG with OpenAI"""
    try:
        supabase = get_supabase_client()
        
        # Build query based on filters
        query = supabase.table("materials").select("*, courses(code, name)")
        
        if request.material_id:
            query = query.eq("id", request.material_id)
            logger.info(f"Filtering by material_id: {request.material_id}")
        elif request.course_id:
            query = query.eq("course_id", request.course_id)
            logger.info(f"Filtering by course_id: {request.course_id}")
        else:
            logger.info("Querying all materials")
        
        result = query.execute()
        materials = result.data or []
        
        logger.info(f"Found {len(materials)} materials")
        
        if not materials:
            return {
                "answer": "âŒ No encontrÃ© materiales con los filtros seleccionados.\n\nğŸ’¡ Consejo: Verifica que haya materiales en ese curso o intenta con 'Todos los cursos'.",
                "sources": [],
                "context": "Sin materiales disponibles"
            }
        
        # Extract text from materials
        all_text = []
        sources = []
        
        for material in materials:
            text = material.get('raw_text', '').strip()
            course = material.get('courses', {})
            
            sources.append({
                'title': material.get('title', 'Sin tÃ­tulo'),
                'course': f"{course.get('code', '')} - {course.get('name', '')}",
                'author': material.get('author', 'Desconocido')
            })
            
            if text:
                all_text.append(text)
                logger.info(f"Material '{material.get('title')}' has {len(text)} characters")
            else:
                logger.warning(f"Material '{material.get('title')}' has NO TEXT in raw_text field")
        
        if not all_text:
            material_names = [m.get('title') for m in materials]
            return {
                "answer": f"âš ï¸ **EncontrÃ© {len(materials)} material(es), pero ninguno tiene contenido de texto.**\n\nğŸ“ Materiales encontrados:\n" + "\n".join(f"â€¢ {name}" for name in material_names) + "\n\nğŸ’¡ **SoluciÃ³n:** Ve a AdministraciÃ³n â†’ Materiales y agrega contenido en el campo 'Contenido/Texto' al crear o editar el material.",
                "sources": sources,
                "context": f"{len(materials)} materiales sin texto procesable"
            }
        
        # Combine all text for context
        combined_text = "\n\n".join(all_text)
        logger.info(f"Total text length: {len(combined_text)} characters")
        
        # === NUEVA IMPLEMENTACIÃ“N: Usar OpenAI para RAG ===
        try:
            # Truncate context if too long (OpenAI has token limits)
            max_context_length = 8000  # Leave room for question and response
            if len(combined_text) > max_context_length:
                combined_text = combined_text[:max_context_length] + "\n\n[...contenido truncado por lÃ­mite de tokens...]"
                logger.info(f"Context truncated to {max_context_length} characters")
            
            # Build system prompt
            system_prompt = f"""Eres un asistente educativo experto que ayuda a estudiantes a entender materiales de cursos.

Tu tarea es responder preguntas basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado de los materiales del curso.

Instrucciones:
1. Responde en espaÃ±ol de forma clara y educativa
2. Si la informaciÃ³n estÃ¡ en el contexto, proporciona una respuesta detallada
3. Si NO estÃ¡ en el contexto, di claramente que no encontraste esa informaciÃ³n
4. Cita fragmentos relevantes del material cuando sea apropiado
5. SÃ© conciso pero completo

CONTEXTO DE LOS MATERIALES:
{combined_text}
"""
            
            # Call OpenAI API
            logger.info(f"Calling OpenAI API with model: {settings.OPENAI_MODEL}")
            
            response = openai.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": request.question}
                ],
                temperature=0.7,
                max_tokens=800,
                top_p=0.9
            )
            
            ai_answer = response.choices[0].message.content
            logger.info(f"OpenAI response received: {len(ai_answer)} characters")
            
            return {
                "answer": ai_answer,
                "sources": sources,
                "context": f"âœ¨ Respuesta generada por IA â€¢ Consultados {len(materials)} materiales â€¢ {len(combined_text)} caracteres de contexto"
            }
            
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API error: {e}")
            # Fallback to keyword search if OpenAI fails
            return await fallback_keyword_search(request.question, combined_text, sources, len(materials))
        
    except Exception as e:
        logger.error(f"Error processing RAG query: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error al procesar la consulta: {str(e)}")


async def fallback_keyword_search(question: str, combined_text: str, sources: list, material_count: int):
    """Fallback search method when OpenAI is unavailable"""
    logger.info("Using fallback keyword search")
    
    question_lower = question.lower()
    combined_text_lower = combined_text.lower()
    
    # Extract keywords
    stop_words = {'que', 'es', 'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'en', 'por', 'para', 'con', 'sobre', 'como', 'quÃ©', 'cuÃ¡l', 'cÃ³mo'}
    keywords = [kw for kw in question_lower.split() if len(kw) > 2 and kw not in stop_words]
    relevant_keywords = [kw for kw in keywords if kw in combined_text_lower]
    
    if not relevant_keywords:
        answer = f"ğŸ¤” **No encontrÃ© coincidencias exactas para tu pregunta.**\n\n"
        answer += f"ğŸ“š **Vista previa del material:**\n{combined_text[:300]}...\n\n"
        answer += "ğŸ’¡ **Nota:** Servicio de IA temporalmente no disponible. Usa bÃºsqueda por palabras clave."
    else:
        sentences = []
        for sep in ['. ', '.\n', '! ', '? ']:
            if sep in combined_text:
                sentences = combined_text.split(sep)
                break
        
        if not sentences:
            sentences = [combined_text]
        
        relevant_sentences = []
        for sentence in sentences:
            if any(kw in sentence.lower() for kw in relevant_keywords):
                relevant_sentences.append(sentence.strip())
                if len(relevant_sentences) >= 3:
                    break
        
        if relevant_sentences:
            answer = "âœ… **InformaciÃ³n encontrada:**\n\n"
            answer += ". ".join(relevant_sentences[:2]) + "."
            answer += f"\n\nğŸ”‘ **Palabras clave:** {', '.join(relevant_keywords)}"
        else:
            answer = f"El material contiene informaciÃ³n relacionada.\n\n"
            answer += f"ğŸ“„ **Fragmento:**\n{combined_text[:300]}..."
    
    return {
        "answer": answer + "\n\nâš ï¸ _Respuesta generada por bÃºsqueda bÃ¡sica (OpenAI no disponible)_",
        "sources": sources,
        "context": f"BÃºsqueda por keywords â€¢ {material_count} materiales consultados"
    }


@router.post("/chat", response_model=dict)
async def chat(request: QueryRequest):
    """Legacy chat endpoint - redirects to query"""
    return await query_materials(request)
