# ü§ñ Implementaci√≥n del Sistema RAG - EduRAG

## üìã Contenido

1. [¬øQu√© es RAG?](#qu√©-es-rag)
2. [Pipeline Completo](#pipeline-completo)
3. [Procesamiento de PDFs](#procesamiento-de-pdfs)
4. [Generaci√≥n de Embeddings](#generaci√≥n-de-embeddings)
5. [B√∫squeda Vectorial](#b√∫squeda-vectorial)
6. [Generaci√≥n de Respuestas](#generaci√≥n-de-respuestas)
7. [Optimizaciones](#optimizaciones)

---

## üéØ ¬øQu√© es RAG?

**RAG (Retrieval-Augmented Generation)** es un patr√≥n arquitect√≥nico que combina:

1. **Retrieval (Recuperaci√≥n):** B√∫squeda de informaci√≥n relevante en una base de conocimiento
2. **Augmented (Aumentado):** Agregar esa informaci√≥n al contexto del LLM
3. **Generation (Generaci√≥n):** LLM genera respuesta basada en el contexto

### RAG vs LLM Puro

**LLM Puro (sin RAG):**

```python
response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "¬øQu√© es el cableado estructurado?"}
    ]
)
# Respuesta basada en conocimiento pre-entrenado (hasta Sep 2023)
# ‚ùå No sabe sobre materiales espec√≠ficos del curso
# ‚ùå Puede "alucinar" informaci√≥n incorrecta
# ‚ùå No cita fuentes verificables
```

**Con RAG:**

```python
# 1. Buscar informaci√≥n relevante
chunks = search_in_database("¬øQu√© es el cableado estructurado?")

# 2. Construir contexto
context = "\n\n".join([chunk.content for chunk in chunks])

# 3. Generar respuesta con contexto
response = openai.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": f"Responde bas√°ndote en este contexto:\n{context}"},
        {"role": "user", "content": "¬øQu√© es el cableado estructurado?"}
    ]
)
# ‚úÖ Respuesta basada en materiales espec√≠ficos del curso
# ‚úÖ Informaci√≥n actualizada y verificable
# ‚úÖ Cita fuentes con t√≠tulos y autores
```

### Ventajas de RAG

| Aspecto | LLM Puro | RAG |
|---------|----------|-----|
| Conocimiento | Hasta fecha de entrenamiento | Actualizable en tiempo real |
| Precisi√≥n | Puede alucinar | Basado en fuentes reales |
| Fuentes | No cita | Cita documentos espec√≠ficos |
| Dominio espec√≠fico | Gen√©rico | Especializado en materiales del curso |
| Costo | $0.15 por 1M tokens | $0.17 por 1M tokens (embedding + chat) |

---

## üîÑ Pipeline Completo

### Diagrama de Flujo

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE 1: INDEXACI√ìN                        ‚îÇ
‚îÇ                  (Una vez por material)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Usuario sube PDF
         ‚îÇ
         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Upload PDF   ‚îÇ  ‚Üê Supabase Storage
    ‚îÇ to Storage   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Extract Text ‚îÇ  ‚Üê pdfplumber
    ‚îÇ from PDF     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Split into   ‚îÇ  ‚Üê LangChain RecursiveTextSplitter
    ‚îÇ Chunks       ‚îÇ     (500 tokens, 50 overlap)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Generate     ‚îÇ  ‚Üê OpenAI text-embedding-3-small
    ‚îÇ Embeddings   ‚îÇ     (1536 dimensions)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Store in DB  ‚îÇ  ‚Üê PostgreSQL + pgvector
    ‚îÇ with Vectors ‚îÇ     HNSW index
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FASE 2: CONSULTA                          ‚îÇ
‚îÇ                  (Cada vez que el usuario pregunta)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Usuario hace pregunta
         ‚îÇ
         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Convert      ‚îÇ  ‚Üê OpenAI text-embedding-3-small
    ‚îÇ Query to     ‚îÇ     (mismo modelo que indexaci√≥n)
    ‚îÇ Embedding    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Vector       ‚îÇ  ‚Üê match_material_chunks() SQL
    ‚îÇ Similarity   ‚îÇ     Cosine similarity > 0.3
    ‚îÇ Search       ‚îÇ     Top 5 chunks
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Build        ‚îÇ  ‚Üê Chunks + metadata
    ‚îÇ Context      ‚îÇ     (course, material, author)
    ‚îÇ Prompt       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Generate     ‚îÇ  ‚Üê OpenAI GPT-4o-mini
    ‚îÇ Answer with  ‚îÇ     Temperature 0.7
    ‚îÇ GPT-4        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Return       ‚îÇ  ‚Üê Answer + sources + similarities
    ‚îÇ to User      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÑ Procesamiento de PDFs

### 1. Extracci√≥n de Texto

**C√≥digo (pdf_processor.py):**

```python
import pdfplumber

def extract_text_from_pdf(file_path: str) -> str:
    """
    Extrae texto completo de un PDF.
    
    pdfplumber es superior a PyPDF2 porque:
    - Detecta layouts complejos (tablas, columnas)
    - Preserva formato y espaciado
    - Funciona con PDFs escaneados (con OCR adicional)
    """
    text_content = []
    
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                text_content.append(text)
    
    return "\n\n".join(text_content)
```

**Ejemplo de Salida:**

```
CAP√çTULO 1: CABLEADO ESTRUCTURADO

1.1 Introducci√≥n

El cableado estructurado es un sistema de cables, conectores...

1.2 Normas y Est√°ndares

Las principales normas que regulan el cableado estructurado son:
- ANSI/TIA-568: Est√°ndar de cableado comercial
- ISO/IEC 11801: Est√°ndar internacional...
```

### 2. Chunking Inteligente

**¬øPor qu√© necesitamos chunks?**

- **Limitaci√≥n de contexto:** GPT-4o-mini tiene ventana de 128K tokens (~96K palabras)
- **Precisi√≥n:** Chunks peque√±os = b√∫squedas m√°s precisas
- **Performance:** Procesar 1000 chunks es m√°s r√°pido que 1 documento gigante

**C√≥digo (pdf_processor.py):**

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

def split_text_into_chunks(text: str) -> List[str]:
    """
    Divide texto usando estrategia recursiva.
    
    Estrategia de divisi√≥n (en orden de prioridad):
    1. P√°rrafos (\n\n) - Preserva contexto sem√°ntico completo
    2. L√≠neas (\n) - Si p√°rrafo es muy largo
    3. Oraciones (. ) - Si l√≠nea es muy larga
    4. Palabras ( ) - Si oraci√≥n es muy larga
    5. Caracteres - √öltimo recurso
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,        # Tama√±o objetivo en caracteres
        chunk_overlap=50,      # Solapamiento entre chunks
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""]
    )
    
    chunks = splitter.split_text(text)
    return chunks
```

**Ejemplo de Chunks:**

```python
text = """
CAP√çTULO 1: INTRODUCCI√ìN

El cableado estructurado es fundamental...

CAP√çTULO 2: NORMAS

Las normas principales son ANSI/TIA-568...
"""

chunks = split_text_into_chunks(text)

# Chunk 0:
"CAP√çTULO 1: INTRODUCCI√ìN\n\nEl cableado estructurado es fundamental..."

# Chunk 1 (con overlap de 50 caracteres):
"...es fundamental para redes modernas.\n\nCAP√çTULO 2: NORMAS\n\nLas normas..."
```

### ¬øPor qu√© chunk_size=500 y overlap=50?

**Tama√±o de Chunk:**

```python
# Muy peque√±o (100 caracteres)
‚ùå "El cableado estructurado es un sistema de cables"
# Falta contexto (¬øqu√© tipo de sistema? ¬øpara qu√©?)

# Muy grande (2000 caracteres)
‚ùå Incluye m√∫ltiples conceptos diferentes
# B√∫squeda menos precisa (trae informaci√≥n irrelevante)

# √ìptimo (500 caracteres)
‚úÖ "El cableado estructurado es un sistema de cables, conectores y 
   dispositivos que proporciona una infraestructura de telecomunicaciones
   flexible para edificios. Est√° regulado por normas ANSI/TIA-568 e 
   ISO/IEC 11801..."
# Contexto completo + enfocado
```

**Overlap:**

```python
# Sin overlap
Chunk 1: "...hasta 10 Gbps."
Chunk 2: "La norma TIA-568..."
# Si buscan "velocidad de TIA-568", no encuentra relaci√≥n

# Con overlap de 50
Chunk 1: "...hasta 10 Gbps. La norma TIA-568..."
Chunk 2: "La norma TIA-568 especifica..."
# Ambos chunks tienen contexto compartido
```

---

## üß† Generaci√≥n de Embeddings

### ¬øQu√© son los Embeddings?

**Embeddings** son representaciones num√©ricas (vectores) de texto que capturan significado sem√°ntico.

**Ejemplo Conceptual:**

```python
# Textos similares ‚Üí vectores cercanos
embedding("gato") = [0.2, 0.8, 0.1, ...]      ‚îÄ‚îê
embedding("felino") = [0.3, 0.7, 0.2, ...]    ‚îÄ‚î§ Cercanos
embedding("cachorro") = [0.25, 0.75, 0.15, ...] ‚îò

embedding("computadora") = [0.9, 0.1, 0.8, ...] ‚Üê Lejano
```

### OpenAI text-embedding-3-small

**Especificaciones:**

- **Dimensiones:** 1536 (configurables: 512 a 3072)
- **Modelo:** Basado en transformer
- **Costo:** $0.02 por 1M tokens (~700K palabras)
- **Performance:** 62.3% en MTEB benchmark

**C√≥digo (pdf_processor.py):**

```python
from openai import OpenAI

def generate_embeddings(texts: List[str], api_key: str) -> List[List[float]]:
    """
    Genera embeddings para lista de textos.
    
    Args:
        texts: Chunks de texto
        api_key: OpenAI API key
    
    Returns:
        Lista de vectores [1536 dimensiones cada uno]
    """
    client = OpenAI(api_key=api_key)
    embeddings = []
    
    # Procesar en lotes de 100 (l√≠mite de OpenAI)
    batch_size = 100
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        response = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch,
            dimensions=1536
        )
        
        batch_embeddings = [item.embedding for item in response.data]
        embeddings.extend(batch_embeddings)
    
    return embeddings
```

**Resultado:**

```python
chunk = "El cableado estructurado es un sistema..."

embedding = generate_embeddings([chunk], api_key)[0]

print(len(embedding))  # 1536
print(embedding[:5])   # [0.0234, -0.0145, 0.0456, -0.0089, 0.0123]
```

### Almacenamiento en PostgreSQL

```python
def store_chunks_in_database(chunks, embeddings, material_id, course_id):
    """
    Inserta chunks con embeddings en material_chunks.
    """
    supabase = get_supabase_client()
    
    records = []
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        records.append({
            "material_id": material_id,
            "course_id": course_id,
            "chunk_index": i,
            "content": chunk,
            "embedding": embedding  # pgvector convierte lista a VECTOR
        })
    
    # Insertar en lotes
    batch_size = 100
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        supabase.table("material_chunks").insert(batch).execute()
```

---

## üîç B√∫squeda Vectorial

### Similitud de Coseno

**F√≥rmula:**

```
cosine_similarity(A, B) = (A ¬∑ B) / (||A|| * ||B||)

Donde:
- A ¬∑ B = producto punto (sum(a_i * b_i))
- ||A|| = magnitud del vector A (sqrt(sum(a_i¬≤)))
```

**Rango:** [-1, 1]
- 1 = Vectores id√©nticos (mismo significado)
- 0 = Vectores ortogonales (sin relaci√≥n)
- -1 = Vectores opuestos

**PostgreSQL con pgvector:**

```sql
-- Operador <=> retorna distancia de coseno (0 a 2)
-- Convertimos a similitud: 1 - distancia
SELECT
    content,
    (1 - (embedding <=> query_vector)) AS similarity
FROM material_chunks
WHERE (1 - (embedding <=> query_vector)) >= 0.3
ORDER BY embedding <=> query_vector
LIMIT 5;
```

### Funci√≥n match_material_chunks

**Uso desde Backend:**

```python
# routers/rag_vector.py

async def get_relevant_chunks(query: str, course_id: str, material_id: str = None):
    """
    Busca chunks relevantes para una consulta.
    """
    # 1. Generar embedding de la pregunta
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=query,
        dimensions=1536
    )
    query_embedding = response.data[0].embedding
    
    # 2. Buscar chunks similares
    supabase = get_supabase_client()
    result = supabase.rpc('match_material_chunks', {
        'query_embedding': query_embedding,
        'course_filter': course_id,
        'material_filter': material_id,
        'match_threshold': 0.3,  # Solo chunks con similitud >= 30%
        'match_count': 5         # Top 5
    }).execute()
    
    chunks = result.data
    logger.info(f"Found {len(chunks)} relevant chunks")
    
    return chunks
```

**Ejemplo de Resultado:**

```python
query = "¬øQu√© normas regulan el cableado estructurado?"
chunks = await get_relevant_chunks(query, course_id, material_id)

# [
#   {
#     "content": "Las principales normas son ANSI/TIA-568 e ISO/IEC 11801...",
#     "similarity": 0.87,
#     "material_title": "Cableado Estructurado",
#     "course_code": "REDES-102",
#     "author": "Dr. Garc√≠a"
#   },
#   {
#     "content": "La norma ANSI/TIA-568 define los est√°ndares para cableado...",
#     "similarity": 0.82,
#     ...
#   },
#   ...
# ]
```

---

## ü§ñ Generaci√≥n de Respuestas

### Construcci√≥n del Prompt

**Estrategia:**

1. **System Message:** Define el rol del asistente
2. **Context:** Chunks recuperados como conocimiento
3. **User Query:** Pregunta original del usuario

**C√≥digo (routers/rag_vector.py):**

```python
async def generate_answer(query: str, chunks: List[dict]):
    """
    Genera respuesta usando GPT-4 con contexto de chunks.
    """
    # Construir contexto desde chunks
    context_parts = []
    for i, chunk in enumerate(chunks):
        context_parts.append(
            f"[Fuente {i+1}: {chunk['material_title']} - {chunk['course_code']}]\n"
            f"{chunk['content']}\n"
            f"(Similitud: {chunk['similarity']:.2f})"
        )
    
    context = "\n\n---\n\n".join(context_parts)
    
    # Prompt estructurado
    system_prompt = f"""Eres un asistente educativo especializado en responder preguntas sobre materiales de cursos.

CONTEXTO RECUPERADO:
{context}

INSTRUCCIONES:
1. Responde SOLO bas√°ndote en el contexto proporcionado
2. Si el contexto no contiene la informaci√≥n, di "No encontr√© informaci√≥n suficiente"
3. Cita las fuentes cuando sea relevante (ej: "Seg√∫n el material de Cableado Estructurado...")
4. S√© claro, conciso y educativo
5. Si hay m√∫ltiples fuentes con informaci√≥n similar, sintet√≠zalas
"""
    
    # Llamada a OpenAI
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": query}
        ],
        temperature=0.7,        # Creatividad moderada
        max_tokens=1000,        # L√≠mite de respuesta
        presence_penalty=0.1,   # Penaliza repetici√≥n
        frequency_penalty=0.1
    )
    
    answer = response.choices[0].message.content
    
    return {
        "answer": answer,
        "sources": [
            {
                "material_title": chunk["material_title"],
                "course_code": chunk["course_code"],
                "similarity": chunk["similarity"],
                "content_preview": chunk["content"][:200] + "..."
            }
            for chunk in chunks
        ]
    }
```

### Ejemplo de Conversaci√≥n

**Input:**

```python
query = "¬øQu√© velocidades soporta el cable Cat6?"
course_id = "uuid-redes-102"
```

**Chunks Recuperados:**

```python
[
  {
    "content": "El cable Cat6 (Categor√≠a 6) soporta velocidades de hasta 1 Gbps 
                a distancias de 100 metros, y puede alcanzar hasta 10 Gbps en 
                distancias m√°s cortas (hasta 55 metros).",
    "similarity": 0.89,
    "material_title": "Cableado Estructurado",
    "course_code": "REDES-102"
  },
  {
    "content": "Comparado con Cat5e que soporta 1 Gbps, el Cat6 mejora el 
                rendimiento con mayor ancho de banda (250 MHz vs 100 MHz).",
    "similarity": 0.75,
    ...
  }
]
```

**Respuesta Generada:**

```
El cable Cat6 (Categor√≠a 6) soporta velocidades de hasta 1 Gbps a distancias 
de 100 metros. En distancias m√°s cortas (hasta 55 metros), puede alcanzar 
velocidades de hasta 10 Gbps.

Esto representa una mejora significativa sobre el Cat5e, que est√° limitado a 
1 Gbps. El Cat6 tambi√©n ofrece mayor ancho de banda (250 MHz comparado con 
100 MHz del Cat5e).

**Fuentes:**
- Material: "Cableado Estructurado" (REDES-102)
- Similitud: 89%
```

---

## ‚ö° Optimizaciones

### 1. Threshold Ajustable

**Problema:** Threshold muy alto = pocas respuestas, muy bajo = mucho ruido

**Soluci√≥n:** Ajustar din√°micamente

```python
async def get_relevant_chunks_adaptive(query, course_id):
    """
    Intenta con threshold alto, si no encuentra baja gradualmente.
    """
    thresholds = [0.7, 0.5, 0.3]
    
    for threshold in thresholds:
        chunks = await search_with_threshold(query, course_id, threshold)
        if len(chunks) >= 3:  # M√≠nimo 3 chunks
            return chunks
    
    # Si a√∫n no encuentra, retornar los mejores disponibles
    return await search_with_threshold(query, course_id, 0.0)
```

### 2. Cach√© de Embeddings

**Problema:** Misma pregunta genera embedding m√∫ltiples veces

**Soluci√≥n:** Redis cache

```python
import redis
import hashlib

redis_client = redis.Redis(host='localhost', port=6379)

async def get_embedding_cached(text: str):
    # Hash del texto como key
    cache_key = f"emb:{hashlib.sha256(text.encode()).hexdigest()}"
    
    # Intentar obtener desde cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Si no est√° en cache, generar
    embedding = await generate_embedding(text)
    
    # Guardar en cache (TTL 1 hora)
    redis_client.setex(cache_key, 3600, json.dumps(embedding))
    
    return embedding
```

### 3. Batch Processing de PDFs

**Problema:** Procesar PDFs uno por uno es lento

**Soluci√≥n:** Background tasks con Celery (futuro)

```python
from celery import Celery

celery_app = Celery('edurag', broker='redis://localhost:6379')

@celery_app.task
def process_pdf_async(file_path, material_id, course_id):
    """
    Procesa PDF en background.
    """
    result = process_pdf_material(file_path, material_id, course_id)
    return result

# En el endpoint
@router.post("/materials/upload")
async def upload_material(file: UploadFile):
    # Guardar archivo
    file_path = save_file(file)
    
    # Crear registro
    material = create_material_record(file_path)
    
    # Procesar en background
    process_pdf_async.delay(file_path, material.id, material.course_id)
    
    return {"message": "Processing started", "material_id": material.id}
```

### 4. Reranking

**Problema:** Similitud vectorial no siempre es perfecta

**Soluci√≥n:** Reordenar resultados con modelo especializado

```python
from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def rerank_chunks(query: str, chunks: List[dict]):
    """
    Reordena chunks usando cross-encoder.
    """
    # Crear pares query-chunk
    pairs = [[query, chunk["content"]] for chunk in chunks]
    
    # Calcular scores
    scores = reranker.predict(pairs)
    
    # Reordenar por score
    for chunk, score in zip(chunks, scores):
        chunk["rerank_score"] = float(score)
    
    chunks.sort(key=lambda x: x["rerank_score"], reverse=True)
    
    return chunks
```

---

## üîó Pr√≥ximo Documento

Finaliza con [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) para aprender a desplegar el sistema en producci√≥n.

---

*√öltima actualizaci√≥n: Octubre 23, 2025*
*Versi√≥n: 1.0.0*
